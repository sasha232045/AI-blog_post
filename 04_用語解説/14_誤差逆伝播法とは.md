難易度: 280

## 誤差逆伝播法とは

**誤差逆伝播法 (Error Backpropagation)**、あるいは単に**バックプロパゲーション**とは、ニューラルネットワークの学習において、現在最も広く用いられているアルゴリズムです。この手法の登場が、今日のディープラーニングの発展を可能にしたと言っても過言ではありません。

その目的は、ニューラルネットワークの**予測値**と、実際の**正解値**との間に生じた**誤差 (Error)** を、ネットワークの各層に逆方向（出力層から入力層へ）に伝播させ、その誤差を最小化するように各ニューロンの**重み (Weight)** や**バイアス (Bias)** を効率的に調整（更新）することです。

### 誤差逆伝播法の仕組み

ニューラルネットワークの学習プロセスは、大きく分けて2つのステップで構成されます。

1.  **順伝播 (Forward Propagation)**: 入力データが、入力層から中間層を経て、出力層へと一方向に流れていくプロセスです。各層のニューロンは、前の層からの出力に自身の重みを掛け、バイアスを足し、活性化関数を適用して次の層へと出力します。こうして、最終的な予測値が計算されます。

2.  **逆伝播 (Backward Propagation)**: ここで誤差逆伝播法が活躍します。まず、出力層で計算された予測値と、あらかじめ用意されている正解値とを比較し、その誤差を算出します。次に、その誤差を「どれくらい各ニューロンがその誤差に貢献したか」に応じて、出力層から入力層方向へと逆向きに伝播させていきます。そして、各ニューロンの重みやバイアスを、「誤差をより小さくする方向」に少しだけ調整するのです。

この「順伝播」と「逆伝播」のサイクルを、大量の学習データを使い、何千回、何百万回と繰り返すことで、ニューラルネットワークは徐々に賢くなり、与えられたタスクに対して高い精度で予測を行えるように成長していきます。

### 微分の連鎖律 (Chain Rule)

誤差逆伝播法の数学的な核心は、高校数学で学ぶ**微分の連鎖律 (Chain Rule)** にあります。

ニューラルネットワーク全体の誤差は、最終出力層の誤差であり、それは中間層のニューロンの出力の関数であり、さらにその中間層のニューロンの出力は、その前の層のニューロンの出力と重みの関数であり…というように、複雑な関数の合成で表現されます。

ある層の重みを少し変化させたときに、最終的な誤差がどれだけ変化するか（=誤差に対する重みの勾配）を知りたい場合、この合成関数の微分を計算する必要があります。連鎖律を用いることで、この複雑な微分計算を、各層ごとの単純な微分の掛け算に分解し、効率的に計算することが可能になります。これにより、どの重みをどちらの方向にどれだけ調整すれば誤差が最も効率的に減少するかが分かるのです。

### まとめ

誤差逆伝播法は、「予測と正解のズレ」という最終結果から、その原因をネットワークの各部分に遡って分配し、パラメータを賢く更新していくための洗練されたアルゴリズムです。この効率的な学習手法の確立が、多層で巨大なニューラルネットワーク（ディープラーニング）の学習を現実的な時間で可能にし、今日のAI技術の飛躍的な進歩を支える基盤技術となっています。