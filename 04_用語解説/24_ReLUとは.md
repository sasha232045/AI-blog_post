難易度: 150

## ReLU とは

**ReLU (Rectified Linear Unit)**、日本語では「**正規化線形ユニット**」とは、現代のディープラーニングにおいて、ニューラルネットワークの**中間層**における活性化関数として、最も広く使われている関数です。そのシンプルさと優れた性能から、事実上の業界標準（デファクトスタンダード）と見なされています。

「レル」あるいは「レルー」と発音されます。

### ReLUの定義

ReLUの定義は、驚くほどシンプルです。

`f(x) = max(0, x)`

これは、数式で書くと上記のようになりますが、要するに以下のルールを意味します。

*   **入力 `x` が 0 より大きい場合**: 出力は入力 `x` と同じ。
*   **入力 `x` が 0 以下の場合**: 出力は `0`。

グラフにすると、原点で折れ曲がった、非常に単純な形状をしています。

### なぜReLUが広く使われるのか？

この単純な関数が、なぜディープラーニングの世界を席巻したのでしょうか。それには、いくつかの明確な理由があります。

1.  **計算が非常に高速**: ReLUの計算は、入力が正か負かを判断し、負であれば0にするだけです。かつて主流だった**シグモイド関数**が、複雑な指数計算を必要としたのと比較して、計算コストが劇的に低減されました。これにより、巨大なニューラルネットワークの学習時間を大幅に短縮することが可能になりました。

2.  **勾配消失問題の緩和**: シグモイド関数が抱えていた最大の課題は、**勾配消失問題**でした。入力の絶対値が大きくなると勾配（傾き）が0に近づき、学習が停滞してしまう問題です。一方、ReLUは、入力が正である限り、その勾配は常に `1` です。これにより、層を深くしても勾配が消えにくく、誤差逆伝播法による学習が効率的に進むようになりました。この性質が、**AlexNet** (2012) に代表されるような、深いネットワーク（ディープラーニング）の成功を可能にした最大の要因です。

3.  **スパース性による汎化性能の向上**: 入力が0以下の場合、ReLUの出力は0になります。これは、ネットワーク内の一部のニューロンが常に非活性化状態になることを意味し、**スパース（疎）**な表現を生み出します。このスパース性が、計算効率の向上に寄与すると同時に、モデルがデータに過剰に適合してしまう**過学習 (Overfitting)** を抑制し、汎化性能を高める効果があると考えられています。

### ReLUの課題：Dying ReLU問題

ReLUにも弱点がないわけではありません。最もよく知られているのが **Dying ReLU（死んだReLU）問題**です。

学習の過程で、あるニューロンへの入力が常に0以下になってしまうと、そのニューロンの出力は常に0となり、勾配も常に0になってしまいます。こうなると、そのニューロンは、それ以降の学習プロセスにおいて、二度と「生き返る」ことができず、重みの更新に全く寄与しなくなってしまいます。これが「死んだReLU」と呼ばれる状態です。

この問題を解決するために、ReLUにはいくつかの派生形が考案されています。

*   **Leaky ReLU**: 入力が負の場合でも、完全に0にするのではなく、`0.01` のような非常に小さな傾きを持たせます。これにより、ニューロンが完全に死んでしまうことを防ぎます。
*   **Parametric ReLU (PReLU)**: Leaky ReLUの負領域の傾きを、学習によって自動的に最適化する手法です。
*   **Exponential Linear Unit (ELU)**: 負の入力に対して、滑らかな曲線で負の値を出力する関数です。

### まとめ

ReLUは、その単純さからは想像もつかないほどの大きなインパクトを、ディープラーニングの世界にもたらしました。計算の高速化と勾配消失問題の克服という二つの大きな貢献により、それまで理論上のものであった「深いネットワーク」の学習を、現実的なものへと変えたのです。Dying ReLUといった課題はありますが、そのシンプルさと強力さから、ReLUは今なお、多くのニューラルネットワークアーキテクチャにおいて、最初に試されるべき、そして最も信頼される活性化関数として君臨しています。