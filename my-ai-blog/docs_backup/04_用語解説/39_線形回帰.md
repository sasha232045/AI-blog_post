難易度: 30

## 線形回帰とは (Linear Regression)

**線形回帰 (Linear Regression)** とは、統計学や機械学習の分野で最も基本的かつ広く用いられる予測モデルの一つです。その目的は、一つまたは複数の**<a href="04_用語解説/42_説明変数.md"><abbr title="予測や説明に使用する入力変数（独立変数）">説明変数</abbr></a>（Explanatory Variable）**と、一つの連続的な**<a href="04_用語解説/43_目的変数.md"><abbr title="予測したい対象の出力変数（従属変数）">目的変数</abbr></a>（Objective Variable）**との間に存在する**直線的な関係（線形関係）**をモデル化し、未知のデータに対する予測を行うことです。

中学校で学ぶ一次関数 `y = ax + b` を思い浮かべると、その本質を理解しやすくなります。

### 線形回帰の基本的な考え方

線形回帰は、データポイントの分布を最もよく表現する一本の「最適」な直線を引こうと試みます。この直線のことを**回帰直線**と呼びます。

例えば、ある都市の「家の広さ（説明変数）」と「家賃（目的変数）」のデータが多数あるとします。一般的に、家が広くなるほど家賃は高くなる傾向があるでしょう。線形回帰は、この関係性を数式で表現します。

`家賃 = a × 広さ + b`

- **`広さ`**: 入力となる説明変数
- **`家賃`**: 予測したい目的変数
- **`a` (傾き)**: 広さが1平方メートル増えるごとに、家賃がどれだけ増えるかを示す係数。**重み**とも呼ばれます。
- **`b` (切片)**: 広さが0の場合の基本的な家賃。**バイアス**とも呼ばれます。

実際のデータは完全に一直線上に並ぶわけではないため、モデルはすべてのデータポイントと直線との**誤差（予測値と実際の値の差）**が、全体として最も小さくなるような、最適な `a` と `b` を見つけ出します。この誤差を最小化するプロセスが、機械学習における「学習」に相当します。

### 種類

線形回帰は、用いる説明変数の数によって2つに大別されます。

1.  **単回帰分析 (Simple Linear Regression)**
    説明変数が一つだけの場合の線形回帰です。上記の「広さ」だけで「家賃」を予測する例がこれにあたります。

2.  **重回帰分析 (Multiple Linear Regression)**
    説明変数が複数ある場合の線形回帰です。例えば、「広さ」だけでなく、「駅からの距離」や「築年数」といった複数の要素を使って、より現実に即した家賃の予測を行う場合などがこれにあたります。数式は `y = a1*x1 + a2*x2 + ... + b` のように拡張されます。

### 線形回帰の利点と限界

**利点**:
-   モデルが非常にシンプルであるため、計算が高速で、結果の解釈が容易です。「どの変数が、どの程度結果に影響を与えているか」を理解しやすいという特徴があります。
-   多くの複雑なアルゴリズムの基礎となっており、まず最初に試すべきベースラインモデルとして非常に有用です。

**限界**:
-   変数間の関係が「線形」であることを前提としているため、非線形な関係（例えば、ある点までは増加するが、その後は減少に転じるような関係）を捉えることはできません。

### まとめ

線形回帰は、そのシンプルさにもかかわらず、非常に強力な予測ツールです。データの中に潜む関係性を直線という分かりやすい形でモデル化することで、ビジネスにおける売上予測から、科学的な分析まで、幅広い分野でその基礎的な力を発揮します。回帰分析を学ぶ上での、まさに第一歩と言えるでしょう。