難易度: 190

## シグモイド関数とは

**シグモイド関数 (Sigmoid Function)** とは、ニューラルネットワークの活性化関数として、かつて広く用いられていた関数の一つです。その名前の通り、ギリシャ文字のシグマ（S）に似た、滑らかな**S字型のカーブ**を描く特徴があります。

数学的には、以下の数式で定義されます。

`f(x) = 1 / (1 + e^(-x))`

ここで `e` はネイピア数（自然対数の底、約2.718）です。

### シグモイド関数の特徴

シグモイド関数には、活性化関数として有用ないくつかの特徴があります。

1.  **出力範囲が (0, 1) に限定される**: 入力 `x` がどのような値であっても（正の無限大から負の無限大まで）、出力は必ず0と1の間の値を取ります。この性質は、出力を**確率**として解釈したい場合に非常に便利です。例えば、ある事象が発生する確率（例: 顧客が商品を購入する確率）を予測するような、**二値分類問題**の出力層でよく利用されてきました。

2.  **滑らかで微分可能**: 関数が滑らかな曲線であるため、どの点でも微分が可能です。これは、勾配を用いて重みを更新する**誤差逆伝播法 (Backpropagation)** を適用する上で、必須の条件となります。初期のパーセプトロンで使われたステップ関数には、この性質がありませんでした。

### シグモイド関数の課題と衰退

これらの利点にもかかわらず、現在では、シグモイド関数がディープニューラルネットワークの**中間層**で使われることは稀になりました。その理由は、いくつかの深刻な課題が明らかになったためです。

#### 勾配消失問題 (Vanishing Gradient Problem)

シグモイド関数の最大の欠点です。グラフを見ると分かるように、入力の値が0から離れて、絶対値が大きくなる（例えば、5以上や-5以下になる）と、関数の傾き（微分値）がほとんど0に近くなってしまいます。

誤差逆伝播法では、この傾き（勾配）を層から層へと掛け合わせていくことで、入力層に近い層の重みを更新します。しかし、シグモイド関数を活性化関数として用いた深いネットワークでは、層を遡るごとに、この「ほぼ0」の勾配が繰り返し掛け合わされることになります。その結果、入力層に近い層に伝わる頃には、勾配がほとんど0になってしまい、重みが全く更新されなくなってしまうのです。これが**勾配消失問題**であり、深いネットワークの学習を著しく困難にする原因となりました。

#### 出力の中心が0ではない

シグモイド関数の出力は、常に正の値（0から1）を取ります。これは、後続の層への入力が常に正の値になることを意味し、学習の収束を遅らせる一因となることが指摘されています。

#### 計算コスト

関数の中に指数関数 `e^(-x)` が含まれているため、後述する**ReLU**のような単純な関数と比較して、計算コストが高いという欠点もあります。

### 現在におけるシグモイド関数の役割

これらの課題から、ディープラーニングの中間層における活性化関数の主役は、ReLUとその派生形に取って代わられました。

しかし、シグモイド関数が完全に使われなくなったわけではありません。前述の通り、出力を0から1の間の確率として扱いたい**二値分類問題の出力層**においては、依然として有効な選択肢の一つです。また、リカレントニューラルネットワーク（RNN）の一種であるLSTMやGRUの内部にある「ゲート」と呼ばれる機構でも、情報の流れを制御するためにシグモイド関数が重要な役割を果たしています。

### まとめ

シグモイド関数は、ニューラルネットワークの歴史において、非線形な活性化関数という概念を導入し、誤差逆伝播法による学習を可能にした、重要なマイルストーンでした。勾配消失問題という大きな壁に突き当たったものの、その基本的な考え方や特定の応用における有用性は、今もなお失われていません。その成功と失敗の歴史は、より優れた活性化関数（ReLUなど）の開発を促し、今日のディープラーニングの発展へと繋がる、貴重な教訓となったのです。