難易度: 4

## 微分の連鎖律 (Chain Rule)

微分の連鎖律（Chain Rule）は、**誤差逆伝播法**の数学的な基礎をなす、微分積分学の基本的な法則です。合成関数（ある関数の結果が別の関数の入力となっている関数）の微分を計算するために用いられます。

### 連鎖律の基本的な考え方

2つの関数 `y = f(u)` と `u = g(x)` があるとします。`x` が変化すると `u` が変化し、`u` が変化すると `y` が変化します。このとき、`x` の小さな変化に対する `y` の最終的な変化率（つまり、合成関数 `y = f(g(x))` の `x` に関する微分）を求めたいと考えます。

連鎖律は、この最終的な変化率が、それぞれの関数の変化率の「積」で表されることを示しています。

数式で表すと以下のようになります。

`dy/dx = dy/du * du/dx`

-   `dy/dx`: `x` の変化に対する `y` の変化率（求めたい最終的な微分）
-   `dy/du`: `u` の変化に対する `y` の変化率
-   `du/dx`: `x` の変化に対する `u` の変化率

これは、複数の関数が「鎖（チェーン）」のようにつながっている場合に、最終的な出力の変化が、途中の各段階の変化の掛け算で伝わっていく様子をイメージさせます。

### なぜディープラーニングで重要なのか？

**ディープラーニング**における学習とは、ニューラルネットワークの出力と正解との誤差を最小化するように、**重み**や**バイアス**といったパラメータを調整するプロセスです。この調整のために、誤差が各パラメータの変化に対してどれくらい影響を受けるか、つまり「誤差のパラメータに関する微分（勾配）」を計算する必要があります。

ニューラルネットワークは、入力層から出力層まで、多数の関数（アフィン変換や**活性化関数**など）が何層にもわたって連なった巨大な合成関数と見なすことができます。

-   入力 `x` が最初の層に入り、出力 `u` が得られる。
-   `u` が次の層に入り、出力 `v` が得られる。
-   ...
-   最終的に、ネットワーク全体の出力 `z` が得られ、それと正解ラベルから誤差 `L` が計算される。

このとき、ある中間層の重み `W` が最終的な誤差 `L` に与える影響（`dL/dW`）を計算したい場合、出力側から入力側に向かって、連鎖律を繰り返し適用していく必要があります。

`dL/dW = (dL/dz) * (dz/dv) * (dv/du) * (du/dW)`

このように、出力層で計算された誤差を、連鎖律を使いながら入力層側へ次々と伝播させていくことで、各パラメータに関する勾配を効率的に計算するアルゴリズムが、**誤差逆伝播法**です。

連鎖律は、ディープラーニングモデルがどのようにして膨大な数のパラメータを学習できるのかを理解するための、鍵となる数学的な概念です。

*[誤差逆伝播法]: ニューラルネットワークの学習において、出力層の誤差を効率的に各パラメータに伝播させるアルゴリズム。
*[ディープラーニング]: 深い階層を持つニューラルネットワークを用いた機械学習の手法。
*[重み]: ニューロン間の接続の強さを示すパラメータ。
*[バイアス]: ニューロンの発火のしやすさを調整するパラメータ。
*[活性化関数]: ニューロンの発火を決定する非線形関数。