難易度: 170

## 活性化関数とは

**活性化関数 (Activation Function)** とは、ニューラルネットワークのニューロンにおいて、入力信号の合計値から、最終的な出力信号を生成する役割を担う関数です。この関数は、ニューロンが受け取った情報を「活性化」させるかどうか、つまり、次のニューロンにどれくらいの強さの信号を伝えるかを決定します。

### 活性化関数の役割：非線形性の導入

活性化関数の最も重要な役割は、ニューラルネットワークに**非線形性 (Non-linearity)** を導入することです。

もし活性化関数が存在せず、ニューロンが単に入力信号の合計をそのまま出力するとしたら、どうなるでしょうか。入力と出力の関係は、常に線形（直線的）な関係、すなわち一次関数 `y = ax + b` のような形でしか表現できなくなります。そして、線形な関数は、何度重ねても（層を深くしても）、結果として得られるのは、やはり一つの大きな線形関数に過ぎません。

これでは、画像の中に写っている猫を認識したり、複雑な文章の意味を理解したりといった、現実世界の多様で複雑な（非線形な）問題を解くことはできません。

活性化関数は、入力の合計値に対して、意図的に非線形な変換を施します。これにより、ニューラルネットワークは、層を重ねるごとに、より複雑で、より抽象的な特徴を学習できるようになり、単純な直線では分離できないような問題にも対応できる、豊かな表現力を獲得するのです。

### 代表的な活性化関数

これまで、様々な種類の活性化関数が考案されてきました。以下に代表的なものを紹介します。

1.  **ステップ関数 (Step Function)**
    最も初期のニューロンモデルであるパーセプトロンで使われました。入力がある閾値を超えれば `1` を、超えなければ `0` を出力するという、非常に単純な関数です。しかし、出力が不連続であるため、現在の主流である勾配ベースの学習手法（誤差逆伝播法）には適していません。

2.  **シグモイド関数 (Sigmoid Function)**
    滑らかなS字カーブを描く関数で、出力を0から1の間の連続的な値に変換します。確率的な出力を得たい場合などに有用で、かつては広く使われていました。しかし、層が深くなると**勾配消失問題**を引き起こしやすく、また計算コストが高いといった欠点から、現在では中間層で使われることは稀になりました。

3.  **ReLU (Rectified Linear Unit)**
    「レル」あるいは「レルー」と読みます。現代のディープラーニングにおいて、中間層の活性化関数として最も広く使われている、事実上の標準（デファクトスタンダード）です。その定義は驚くほどシンプルで、「入力が0以下なら0を、0より大きければ入力をそのまま出力する」というものです。計算が非常に高速で、勾配消失問題を起こしにくいという大きな利点があり、ディープラーニングの成功を支える重要な要素技術の一つとなりました。

4.  **ソフトマックス関数 (Softmax Function)**
    主に出力層で用いられる活性化関数です。特に、複数のクラス（カテゴリ）の中から一つを選択する**多クラス分類問題**で活躍します。この関数は、複数のニューロンの出力を正規化し、全ての出力の合計が1となるような、確率分布に変換する機能を持っています。例えば、画像が「犬」「猫」「鳥」のいずれであるかを分類する場合、それぞれのクラスに属する確率（例: 犬 80%, 猫 15%, 鳥 5%）を出力するために使われます。

### まとめ

活性化関数は、ニューラルネットワークに命を吹き込み、単なる線形モデルから、複雑な問題を解く能力を持つ強力なツールへと変貌させる、極めて重要な要素です。どの活性化関数を選択するかは、ネットワークの学習効率や最終的な性能に大きな影響を与えます。特にReLUの登場は、ディープラーニングを実用的なものにする上で、決定的な役割を果たしました。活性化関数は、ニューラルネットワークの表現力を決定づける、鍵となる技術なのです。