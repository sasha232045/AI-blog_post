難易度: 160

## 過学習 (overtraining・オーバーフィッティング) とは

**過学習（Overfitting）** とは、人工知能、特に機械学習モデルを訓練する過程で発生する、非常に一般的かつ重要な問題です。これは、モデルが**訓練データ（Training Data）** に過剰に適合・最適化されすぎた結果、**未知の新しいデータ（テストデータや実世界のデータ）** に対しては、かえって予測精度が低下してしまう現象を指します。

### 「勉強熱心だが、応用力のない生徒」の比喩

過学習は、しばしば「勉強熱心だが、応用力のない生徒」に例えられます。

- この生徒は、教科書や問題集に出てくる問題を、答えごと丸暗記してしまいます。そのため、その問題集と全く同じ問題が出題される定期テストでは、満点を取ることができます。
- しかし、少し問題の形式を変えられたり、初見の問題が出題されたりする模試や実力テストでは、全く歯が立ちません。

この生徒のように、過学習に陥ったAIモデルは、訓練データの「本質的なパターンや法則」ではなく、「個別のデータに含まれるノイズや、偶然の偏り」までをも完璧に学習（暗記）してしまいます。その結果、訓練データに対する正解率は非常に高くなりますが、汎用性（汎化性能）を失い、未知のデータに対しては正しい判断を下せなくなってしまうのです。

### 過学習はなぜ起こるのか？

過学習が起こる主な原因は以下の通りです。

- **複雑すぎるモデル**: モデルの表現力（パラメータの数など）が高すぎる場合、訓練データのノイズまで無理やり学習しようとしてしまいます。
- **訓練データ不足**: 学習するべきデータが少なすぎる場合、データの本質的な傾向を掴むことができず、個々のデータの特徴に過度に依存してしまいます。
- **訓練のやりすぎ**: 同じ訓練データを何度も繰り返し学習させすぎると、モデルがデータを「暗記」し始めてしまいます。

### 過学習への対策

過学習は、機械学習モデルを実用化する上で避けては通れない課題であり、これを防ぐために様々な技術が研究されています。

- **訓練データを増やす**: 最も根本的で効果的な対策です。データの量が増えれば、ノイズや偏りが相対的に薄まり、モデルはより本質的なパターンを学習しやすくなります。
- **モデルを単純にする**: モデルの構造をシンプルにしたり、パラメータの数を減らしたりすることで、ノイズを学習しにくくします。
- **正則化 (Regularization)**: モデルの複雑さにペナルティを課すことで、過剰な学習を抑制する数学的な手法です。
- **ドロップアウト (Dropout)**: 訓練中に、ニューラルネットワークの一部をランダムに無効化することで、モデルが特定のニューロンに過度に依存するのを防ぎます。
- **早期終了 (Early Stopping)**: 訓練データに対する精度が向上し続けても、検証用データ（訓練データとは別）に対する精度が悪化し始めた時点で、訓練を打ち切る手法です。

### 未学習（Underfitting）との関係

過学習の対極にあるのが、**未学習（Underfitting）** です。これは、モデルが単純すぎるか、訓練が不十分なために、訓練データの特徴さえも十分に捉えきれていない状態を指します。モデルの性能を最大化するためには、この未学習と過学習の間の、ちょうど良いバランス（バイアスとバリアンスのトレードオフ）を見つけることが極めて重要となります。