難易度: 3

## 特徴抽出

特徴抽出（Feature Extraction）は、**次元削減**の主要なアプローチの一つです。元の特徴量を変換・射影することによって、データの情報をできるだけ保持したまま、元の次元よりも低い次元の新しい特徴量を生成するプロセスを指します。

**特徴選択**が元の特徴量のサブセットを選ぶのに対し、特徴抽出は元の特徴量を組み合わせて全く新しい特徴量を作り出すという点が根本的に異なります。

### 特徴抽出の目的

-   **次元削減**: **次元の呪い**を回避し、計算効率を向上させます。
-   **ノイズの除去**: 元のデータに含まれるノイズを低減し、より本質的なデータ構造を捉えます。
-   **可視化**: 高次元のデータを2次元や3次元に圧縮することで、人間が直感的に理解できる形でプロットし、データの分布や関係性を視覚的に把握します。

### 代表的な手法

特徴抽出には様々なアルゴリズムが存在しますが、最も広く知られているのが**主成分分析（PCA）**です。

-   **主成分分析 (Principal Component Analysis, PCA)**
    -   データ全体の分散が最も大きくなる方向（第一主成分）、次にその方向と直交する方向で分散が大きくなる方向（第二主成分）…というように、互いに直交する新しい軸（主成分）を見つけ出します。
    -   これらの主成分は、元の特徴量の線形結合で表されます。
    -   通常、最初のいくつかの主成分だけでデータの分散の大部分を説明できるため、これらの主成分を新しい特徴量として採用することで、次元を効果的に削減できます。
    -   PCAは教師なし学習の手法であり、データのラベル情報を利用しません。

-   **線形判別分析 (Linear Discriminant Analysis, LDA)**
    -   教師あり学習の手法であり、異なるクラス（ラベル）のデータを最もよく分離できるような新しい軸を見つけ出します。
    -   分類問題において、クラス間の分離を最大化し、クラス内の分散を最小化する方向へデータを射影します。

-   **t-SNE (t-distributed Stochastic Neighbor Embedding)**
    -   主に高次元データの可視化に用いられる手法です。
    -   元の高次元空間におけるデータ点間の類似度と、低次元空間における類似度が近くなるように、データ点を配置します。
    -   PCAとは異なり、大域的な構造よりも局所的な構造を保持することに優れています。

### 特徴抽出の応用例

-   **画像認識**: 画像データからエッジやテクスチャなどの特徴を抽出し、より少ない情報で画像を表現する。
-   **自然言語処理**: テキストデータから単語の出現頻度や共起関係などを基に、文章の意味的な特徴をベクトルとして抽出する（例: Word2Vec, TF-IDF）。
-   **音声認識**: 音声信号から周波数成分などの音響的特徴を抽出する。

特徴抽出は、生のデータから機械学習モデルが学習しやすい、より有益な表現を生成するための強力なツールです。

*[次元削減]: データの持つ情報をできるだけ損なわずに、特徴量の数を減らすこと。
*[特徴選択]: 多数の特徴量の中から、モデルの性能向上に寄与する特徴量を選び出すプロセス。
*[次元の呪い]: データセットの次元が増えるにつれて、分析が困難になる現象。
*[主成分分析]: データの特徴を要約する新しい軸（主成分）を見つけ出す次元削減手法。