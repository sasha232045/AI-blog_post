難易度: 4

## 勾配消失問題

勾配消失問題（Vanishing Gradient Problem）は、深い階層を持つ**ディープラーニング**モデルを**誤差逆伝播法**で学習させる際に発生する深刻な問題です。出力層から入力層に向かって勾配を伝播させていく過程で、その勾配が指数関数的に小さくなり、最終的にほぼゼロ（消失）に近づいてしまう現象を指します。

### なぜ勾配が消失するのか？

この問題の主な原因は、**活性化関数**として**シグモイド関数**やハイパボリックタンジェント（tanh）関数を繰り返し使用することにありました。

1.  **活性化関数の微分の値**
    *   シグモイド関数の微分（導関数）を考えると、その最大値は入力が0のときの0.25です。入力の絶対値が大きくなるにつれて、微分の値は急激に0に近づきます。

2.  **連鎖律による勾配の乗算**
    *   誤差逆伝播法では、**微分の連鎖律**に従って、各層の活性化関数の微分が次々と掛け合わされていきます。
    *   深いネットワークでは、この「1より小さい（多くの場合はるかに小さい）値」が何層にもわたって繰り返し乗算されることになります。
    *   例えば、0.25が10回掛け合わされると、約0.00000095となり、勾配は極めて小さな値になってしまいます。

### 勾配消失の影響

-   **学習の停滞**: 入力層に近い層（初期層）に伝わる勾配がほぼゼロになるため、これらの層の**重み**パラメータがほとんど更新されなくなります。これにより、ネットワーク全体の学習が停滞し、モデルの性能が向上しなくなります。
-   **表現力の低下**: 初期層は、画像のエッジやテクスチャといった低レベルな特徴を学習する重要な役割を担っています。これらの層が学習できないと、ネットワークはデータから複雑な特徴を捉えることができず、表現力の低いモデルになってしまいます。

### 勾配消失問題への対策

この問題を克服するために、様々な技術が開発されてきました。これらが、ディープラーニングの発展を大きく後押ししました。

1.  **活性化関数 ReLU の使用**
    *   **ReLU (Rectified Linear Unit)** は、入力が正の領域では常に微分が1となるため、勾配が乗算によって小さくなるのを防ぎます。これは、勾配消失問題に対する最もシンプルかつ効果的な解決策の一つであり、現代の多くのネットワークで標準的に使われています。

2.  **残差接続 (Residual Connections)**
    *   **ResNet**で導入された画期的なアーキテクチャです。層を飛び越えて入力信号を出力に直接加算する「ショートカット接続」を設けます。これにより、勾配が消失することなく、深い層まで直接伝播するためのバイパス経路が確保されます。

3.  **バッチ正規化 (Batch Normalization)**
    *   各層の入力の分布を、平均0、分散1に正規化する手法です。これにより、活性化関数の入力が飽和領域（微分が0に近くなる領域）に入るのを防ぎ、勾配の流れを安定させることができます。

4.  **勾配クリッピング (Gradient Clipping)**
    *   勾配が爆発する問題（勾配爆発問題）と対になる対策ですが、勾配のノルムが特定の閾値を超えた場合に、その値を強制的に小さくすることで、学習の安定化を図ります。

*[ディープラーニング]: 深い階層を持つニューラルネットワークを用いた機械学習の手法。
*[誤差逆伝播法]: 出力層の誤差を効率的に各パラメータに伝播させるアルゴリズム。
*[活性化関数]: ニューロンの発火を決定する非線形関数。
*[シグモイド関数]: S字型の曲線を描く活性化関数。
*[微分の連鎖律]: 合成関数の微分を計算するための法則。
*[重み]: ニューロン間の接続の強さを示すパラメータ。
*[ReLU]: 入力が0以下なら0、0より大きければそのまま出力する活性化関数。
*[ResNet]: 残差接続を導入し、非常に深いネットワークの学習を可能にしたモデル。